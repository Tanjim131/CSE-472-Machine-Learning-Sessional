{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Decision Tree ------------------------ \n",
      "Telco_train_accuracy:  98.52680156194533\n",
      "Telco_train_tpr:  95.39473684210526\n",
      "Telco_train_tnr:  99.68400583373845\n",
      "Telco_train_ppv:  99.11141490088859\n",
      "Telco_train_fdr:  0.8885850991114164\n",
      "Telco_train_f1:  97.21756620851491\n",
      "\n",
      "Telco_test_accuracy:  75.1596877217885\n",
      "Telco_test_tpr:  53.29512893982808\n",
      "Telco_test_tnr:  82.35849056603773\n",
      "Telco_test_ppv:  49.865951742627345\n",
      "Telco_test_fdr:  50.134048257372655\n",
      "Telco_test_f1:  51.52354570637119\n",
      "\n",
      "------------------------  AdaBoost ------------------------ \n",
      "Number of boosting rounds, k = 5\n",
      "Telco train_accuracy:  77.10330138445154\n",
      "Telco test_accuracy:  79.347054648687\n",
      "\n",
      "Number of boosting rounds, k = 10\n",
      "Telco train_accuracy:  79.30422435214768\n",
      "Telco test_accuracy:  80.90844570617459\n",
      "\n",
      "Number of boosting rounds, k = 15\n",
      "Telco train_accuracy:  79.71246006389777\n",
      "Telco test_accuracy:  80.69552874378992\n",
      "\n",
      "Number of boosting rounds, k = 20\n",
      "Telco train_accuracy:  80.24494142705005\n",
      "Telco test_accuracy:  81.12136266855926\n",
      "\n",
      "------------------------ Decision Tree ------------------------ \n",
      "Adult_train_accuracy:  90.4333404993704\n",
      "Adult_train_tpr:  72.15916337201888\n",
      "Adult_train_tnr:  96.22977346278317\n",
      "Adult_train_ppv:  85.85735963581183\n",
      "Adult_train_fdr:  14.142640364188164\n",
      "Adult_train_f1:  78.41452428799113\n",
      "\n",
      "Adult_test_accuracy:  83.11528775873718\n",
      "Adult_test_tpr:  55.20020800832033\n",
      "Adult_test_tnr:  91.74909529553679\n",
      "Adult_test_ppv:  67.41822800889172\n",
      "Adult_test_fdr:  32.58177199110829\n",
      "Adult_test_f1:  60.700500357398134\n",
      "\n",
      "------------------------  AdaBoost ------------------------ \n",
      "Number of boosting rounds, k = 5\n",
      "Adult train_accuracy:  82.00608089432143\n",
      "Adult test_accuracy:  82.17554204287207\n",
      "\n",
      "Number of boosting rounds, k = 10\n",
      "Adult train_accuracy:  83.92555511194374\n",
      "Adult test_accuracy:  84.0550334746023\n",
      "\n",
      "Number of boosting rounds, k = 15\n",
      "Adult train_accuracy:  84.3094499554682\n",
      "Adult test_accuracy:  84.1041705054972\n",
      "\n",
      "Number of boosting rounds, k = 20\n",
      "Adult train_accuracy:  84.70870059273365\n",
      "Adult test_accuracy:  84.4911246237946\n",
      "\n",
      "------------------------ Decision Tree ------------------------ \n",
      "Credit Card_train_accuracy:  99.9604994623538\n",
      "Credit Card_train_tpr:  76.5625\n",
      "Credit Card_train_tnr:  100.0\n",
      "Credit Card_train_ppv:  100.0\n",
      "Credit Card_train_fdr:  0.0\n",
      "Credit Card_train_f1:  86.72566371681415\n",
      "\n",
      "Credit Card_test_accuracy:  99.9403110845827\n",
      "Credit Card_test_tpr:  72.22222222222221\n",
      "Credit Card_test_tnr:  99.99296443522003\n",
      "Credit Card_test_ppv:  95.1219512195122\n",
      "Credit Card_test_fdr:  4.878048780487809\n",
      "Credit Card_test_f1:  82.10526315789473\n",
      "\n",
      "------------------------  AdaBoost ------------------------ \n",
      "Number of boosting rounds, k = 5\n",
      "Credit Card train_accuracy:  99.86657596172836\n",
      "Credit Card test_accuracy:  99.86482216214318\n",
      "\n",
      "Number of boosting rounds, k = 10\n",
      "Credit Card train_accuracy:  99.86657596172836\n",
      "Credit Card test_accuracy:  99.86482216214318\n",
      "\n",
      "Number of boosting rounds, k = 15\n",
      "Credit Card train_accuracy:  99.88413175623779\n",
      "Credit Card test_accuracy:  99.89115550718023\n",
      "\n",
      "Number of boosting rounds, k = 20\n",
      "Credit Card train_accuracy:  99.88457065110052\n",
      "Credit Card test_accuracy:  99.88939995084442\n",
      "\n",
      "Wall time: 8h 36min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "import math\n",
    "import sys\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "\n",
    "def impute_numerical(data, column_name, missing_value_indicator):\n",
    "    data.replace({missing_value_indicator: np.nan}, inplace=True)\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    return imputer.fit_transform(data[column_name].values.reshape(-1, 1)) \n",
    "\n",
    "def impute_categorical(data, column_name, missing_value_indicator):\n",
    "    data.replace({missing_value_indicator: np.nan}, inplace=True)\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    return imputer.fit_transform(data[column_name].values.reshape(-1, 1)) \n",
    "\n",
    "def calculate_IQR(data, column_name):\n",
    "    Q1 = data[column_name].quantile(0.25)\n",
    "    Q3 = data[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    if(IQR == 0):\n",
    "        IQR = stats.median_absolute_deviation(data[column_name].values, scale=1)\n",
    "    return IQR\n",
    "\n",
    "def freedman_diaconis(data, column_name):\n",
    "    column_IQR = calculate_IQR(data, column_name)\n",
    "    num_observations = len(data)\n",
    "    column_bin_width = 2 * column_IQR * (num_observations ** (-1 / 3))\n",
    "    column_values = data[column_name].values\n",
    "    column_bins = math.ceil((np.max(column_values) - np.min(column_values)) / column_bin_width)\n",
    "    return column_bins\n",
    "\n",
    "def discretization(data, column_name):\n",
    "    column_bins = freedman_diaconis(data, column_name)\n",
    "    column_discretizer = KBinsDiscretizer(n_bins=column_bins, encode='ordinal', strategy='quantile')\n",
    "    return column_discretizer.fit_transform(data[column_name].values.reshape(-1, 1)).astype(np.int32)\n",
    "\n",
    "def calculate_gini_index(left_node, right_node):\n",
    "    lnp, lnn = left_node\n",
    "    rnp, rnn = right_node\n",
    "    \n",
    "    left_node_total = lnp + lnn\n",
    "    right_node_total = rnp + rnn\n",
    "    \n",
    "    left_node_gini = 1 - (lnp/left_node_total)**2 - (lnn/left_node_total)**2\n",
    "    right_node_gini = 1 - (rnp/right_node_total)**2 - (rnn/right_node_total)**2\n",
    "    \n",
    "    total_gini = (left_node_total * left_node_gini + right_node_total * right_node_gini)/(left_node_total + right_node_total)\n",
    "    \n",
    "    return total_gini\n",
    "\n",
    "def get_threshold_value(data, column_name, target_feature):\n",
    "    MAX_SPLIT_POINTS = 1000\n",
    "    \n",
    "    sorted_values = np.unique(data[column_name])\n",
    "    possible_splitting_values = []\n",
    "    \n",
    "    if len(sorted_values) > MAX_SPLIT_POINTS:\n",
    "        indexes = range(0, len(sorted_values) - 1, math.ceil(len(sorted_values) / MAX_SPLIT_POINTS))\n",
    "    else:\n",
    "        indexes = range(0, len(sorted_values) - 1)\n",
    "    \n",
    "    for i in indexes:\n",
    "        possible_splitting_values.append((sorted_values[i] + sorted_values[i + 1])/2)\n",
    "        \n",
    "    minimum_gini_index = 999 # start with maximum value\n",
    "    best_split_value = None\n",
    "        \n",
    "    for split_value in possible_splitting_values:\n",
    "        left_node_positive = len(data[(data[column_name] <= split_value) & (data[target_feature] == 1)])\n",
    "        left_node_negative = len(data[(data[column_name] <= split_value) & (data[target_feature] == 0)])\n",
    "        \n",
    "        right_node_positive = len(data[(data[column_name] > split_value) & (data[target_feature] == 1)])\n",
    "        right_node_negative = len(data[(data[column_name] > split_value) & (data[target_feature] == 0)])\n",
    "        \n",
    "        current_gini_index = calculate_gini_index([left_node_positive, left_node_negative], [right_node_positive, right_node_negative])\n",
    "        \n",
    "        if minimum_gini_index > current_gini_index:\n",
    "            minimum_gini_index = current_gini_index\n",
    "            best_split_value = split_value\n",
    "            \n",
    "    return best_split_value\n",
    "\n",
    "def binarization(data, column_name, target_feature):\n",
    "    threshold_value = get_threshold_value(data, column_name, target_feature)\n",
    "    column_binarizer = Binarizer(threshold=threshold_value,copy=False)\n",
    "    return column_binarizer.fit_transform(data[column_name].values.reshape(-1,1))\n",
    "\n",
    "def entropy(data, feature):\n",
    "    elements, counts = np.unique(data[feature], return_counts=True)\n",
    "    # print(len(data[feature]), np.sum(counts))\n",
    "    feature_entropy = 0\n",
    "    for i in range(len(elements)):\n",
    "        term = counts[i] / len(data[feature])\n",
    "        feature_entropy += - term * np.log2(term)\n",
    "    return feature_entropy\n",
    "\n",
    "\n",
    "def information_gain(data, split_feature, target_feature):\n",
    "    total_entropy = entropy(data, target_feature)\n",
    "\n",
    "    elements, counts = np.unique(data[split_feature], return_counts=True)\n",
    "\n",
    "    split_feature_entropy = 0\n",
    "    for i in range(len(elements)):\n",
    "        split_feature_entropy += (counts[i] / len(data[split_feature])) * \\\n",
    "                                 entropy(data.where(data[split_feature] == elements[i]).dropna(), target_feature)\n",
    "    return total_entropy - split_feature_entropy\n",
    "\n",
    "\n",
    "def plurality_value(data, target_feature):\n",
    "    return np.unique(data[target_feature])[np.argmax(np.unique(data[target_feature], return_counts=True)[1])]\n",
    "\n",
    "\n",
    "def decision_tree_learning(data, features, target_feature, MAX_DEPTH = 20, parent_data = None , depth = 0):\n",
    "    if len(data) == 0:\n",
    "        # if data is empty then return max occurrence of target_feature in parent_data\n",
    "        return plurality_value(parent_data, target_feature)\n",
    "    elif len(np.unique(data[target_feature])) == 1:\n",
    "        # if all examples have same classification, return the classification\n",
    "        return np.unique(data[target_feature])[0]\n",
    "    elif len(features) == 0 or depth == MAX_DEPTH:\n",
    "        # if features is empty, then return max occurrence of target_feature in data\n",
    "        return plurality_value(data, target_feature)\n",
    "    else:\n",
    "        # none of the above conditions were hit, recursively grow the tree\n",
    "        # for every feature, calculate information gain\n",
    "        information_gain_values = [information_gain(data, feature, target_feature) for feature in features]\n",
    "        best_feature_index = np.argmax(information_gain_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "        # print(best_feature)\n",
    "\n",
    "        # Create the tree structure.\n",
    "        # The root gets the name of the feature (best_feature) with the maximum information\n",
    "        decision_tree = {best_feature: {}}\n",
    "\n",
    "        # Remove the best feature\n",
    "        updated_features = np.delete(features, np.where(features == best_feature))\n",
    "        # print(features)\n",
    "\n",
    "        # Grow a branch under the root node for each possible value of the root node feature\n",
    "\n",
    "        for feature_value in np.unique(data[best_feature]):\n",
    "            # print(feature_value)\n",
    "            sub_data = data.where(data[best_feature] == feature_value).dropna()\n",
    "            sub_tree = decision_tree_learning(sub_data, updated_features, target_feature, MAX_DEPTH, data, depth + 1)\n",
    "            decision_tree[best_feature][feature_value] = sub_tree\n",
    "        \n",
    "        decision_tree[best_feature]['Value Not Found'] = plurality_value(data, target_feature)\n",
    "\n",
    "        return decision_tree\n",
    "    \n",
    "    \n",
    "def predict(test_query, decision_tree):\n",
    "    for key in test_query.keys():\n",
    "        if key in decision_tree.keys():\n",
    "            try:\n",
    "                sub_tree = decision_tree[key][test_query[key]]\n",
    "            except:\n",
    "                return decision_tree[key]['Value Not Found']\n",
    "            \n",
    "            sub_tree = decision_tree[key][test_query[key]]\n",
    "            \n",
    "            if isinstance(sub_tree, dict):\n",
    "                # an intermediate node\n",
    "                return predict(test_query, sub_tree)\n",
    "            else: \n",
    "                # a leaf node\n",
    "                return sub_tree\n",
    "    \n",
    "def make_tree(train_dataset, target_feature, MAX_DEPTH):\n",
    "    return decision_tree_learning(train_dataset, np.array(train_dataset.columns[:-1]), target_feature, MAX_DEPTH) \n",
    "\n",
    "\n",
    "def split_dataset(data):\n",
    "    return train_test_split(data, test_size=0.2, random_state=1505082, shuffle=True)\n",
    "\n",
    "\n",
    "def dt_predictions(data, target_feature, tree):\n",
    "    queries = data.iloc[:,:-1].to_dict(orient='records')\n",
    "    predictions = []\n",
    "    for i in range(len(data)):\n",
    "        predictions.append(predict(queries[i], tree))\n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "def get_label_counts(labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    bins = unique_labels.searchsorted(labels)\n",
    "    return np.bincount(bins)\n",
    "    \n",
    "def calculate_metrics_dt(labels, predictions):\n",
    "    counts = get_label_counts(labels)\n",
    "    real_negatives, real_positives = counts\n",
    "    \n",
    "    true_positives = len(np.where((labels == 1) & (predictions == 1))[0])\n",
    "    false_positives = len(np.where((labels == 0) & (predictions == 1))[0])\n",
    "    \n",
    "    true_negatives = len(np.where((labels == 0) & (predictions == 0))[0])\n",
    "    false_negatives = len(np.where((labels == 1) & (predictions == 0))[0])\n",
    "    \n",
    "    TPR = true_positives/real_positives\n",
    "    TNR = true_negatives/real_negatives\n",
    "    \n",
    "    PPV = true_positives/(true_positives + false_positives)\n",
    "    FDR = 1 - PPV\n",
    "    \n",
    "    F1 = 2 * PPV * TPR / (PPV + TPR)\n",
    "    \n",
    "    return TPR, TNR, PPV, FDR, F1\n",
    "\n",
    "def test_dt(data, target_feature, tree):\n",
    "    # 1 - positive, 0 - negative\n",
    "    predictions = dt_predictions(data, target_feature, tree)\n",
    "    accuracy = metrics.accuracy_score(data[target_feature], predictions)\n",
    "    TPR, TNR, PPV, FDR, F1 = calculate_metrics_dt(data[target_feature].values, predictions)\n",
    "    report_metrics = (accuracy, TPR, TNR, PPV, FDR, F1) \n",
    "    report_metrics = (metric * 100 for metric in report_metrics)\n",
    "    return report_metrics\n",
    "\n",
    "#################################################################################################\n",
    "    \n",
    "def Resample(data, weight_function, random_state):\n",
    "    return data.sample(frac=1, replace=True, weights=weight_function, random_state=random_state)\n",
    "\n",
    "\n",
    "def adaboost(data, features, target_feature, l_weak, k):\n",
    "    random_state = np.random.RandomState(1505082)\n",
    "    weights = np.full(len(data), 1/len(data))\n",
    "    h = []\n",
    "    z = []\n",
    "    for i in range(k):\n",
    "        resampled_data = Resample(data, weights, random_state)\n",
    "        \n",
    "        error = 0\n",
    "        \n",
    "        decision_stump = l_weak(resampled_data, features, target_feature, 1)\n",
    "        decision_stump_predictions = dt_predictions(data, target_feature, decision_stump)\n",
    "        \n",
    "        query_index = 0\n",
    "    \n",
    "        for index, row in data.iterrows():\n",
    "            if (decision_stump_predictions[query_index] != data.at[index, target_feature]):\n",
    "                error = error + weights[query_index]\n",
    "            query_index = query_index + 1\n",
    "\n",
    "        if error > 0.5:\n",
    "            continue\n",
    "            \n",
    "        h.append(decision_stump)\n",
    "        \n",
    "        query_index = 0\n",
    "        for index, row in data.iterrows():\n",
    "            if (decision_stump_predictions[query_index] == data.at[index, target_feature]):\n",
    "                weights[query_index] = weights[query_index] * (error / (1 - error))\n",
    "            query_index = query_index + 1\n",
    "            \n",
    "        weights = weights / np.sum(weights)\n",
    "        \n",
    "        z.append(np.log2((1 - error) / error))\n",
    "    \n",
    "    return h,z\n",
    "\n",
    "\n",
    "def adaboost_predictions(data, target_feature, hypotheses, hypotheses_weights):\n",
    "    decisions = []\n",
    "    queries = data.iloc[:,:-1].to_dict(orient='records')\n",
    "    count = 0\n",
    "    for i in range(len(data)):\n",
    "        decision = 0\n",
    "        for j in range(len(hypotheses)):\n",
    "            prediction = predict(queries[i], hypotheses[j])\n",
    "            if prediction == 1:\n",
    "                decision = decision + hypotheses_weights[j]\n",
    "            else:\n",
    "                decision = decision - hypotheses_weights[j]\n",
    "        \n",
    "        decisions.append(decision)\n",
    "    \n",
    "    return decisions\n",
    "\n",
    "\n",
    "def test_adaboost(data, target_feature, hypotheses, hypotheses_weights):\n",
    "    decisions = adaboost_predictions(data, target_feature, hypotheses, hypotheses_weights)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(data)):            \n",
    "        if (decisions[i] > 0 and data[target_feature].iloc[i] == 1) or (decisions[i] < 0 and data[target_feature].iloc[i] == 0):\n",
    "            count = count + 1\n",
    "    accuracy = (count / len(data)) * 100\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "def preprocess_telco():\n",
    "    data = pd.read_csv(\"telco.csv\")\n",
    "    telco_target_feature = 'Churn'\n",
    "    \n",
    "    labelencoder = LabelEncoder()\n",
    "    data[telco_target_feature] = labelencoder.fit_transform(data[telco_target_feature])\n",
    "    \n",
    "    data.drop('customerID', axis=1, inplace=True)\n",
    "\n",
    "    data['TotalCharges'] = impute_numerical(data, 'TotalCharges', ' ')\n",
    "    \n",
    "    data['TotalCharges'] = discretization(data, 'TotalCharges')\n",
    "    data['MonthlyCharges'] = discretization(data, 'MonthlyCharges')\n",
    "    data['tenure'] = discretization(data, 'tenure')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_adult(file_type):\n",
    "    adult_features = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'label']\n",
    "    adult_target_feature = 'label'\n",
    "    \n",
    "    data = None\n",
    "    if(file_type == 'Train'):\n",
    "        data = pd.read_csv(\"adult.data\", names=adult_features)\n",
    "    else:\n",
    "        data = pd.read_csv(\"adult.test\", skiprows = 1, names=adult_features)\n",
    "    \n",
    "    data_obj = data.select_dtypes(['object'])\n",
    "    data[data_obj.columns] = data_obj.apply(lambda x: x.str.strip())\n",
    "    \n",
    "    labelencoder = LabelEncoder()\n",
    "    data[adult_target_feature] = labelencoder.fit_transform(data[adult_target_feature])\n",
    "    \n",
    "    data['workclass'] = impute_categorical(data, 'workclass', '?')\n",
    "    data['occupation'] = impute_categorical(data, 'occupation', '?')\n",
    "    data['native-country'] = impute_categorical(data, 'native-country', '?')\n",
    "    \n",
    "    data['age'] = binarization(data, 'age', adult_target_feature)\n",
    "    data['fnlwgt'] = binarization(data, 'fnlwgt', adult_target_feature)\n",
    "    data['education-num'] = binarization(data, 'education-num', adult_target_feature)\n",
    "    data['capital-gain'] = binarization(data, 'capital-gain', adult_target_feature)\n",
    "    data['capital-loss'] = binarization(data, 'capital-loss', adult_target_feature)\n",
    "    data['hours-per-week'] = binarization(data, 'hours-per-week', adult_target_feature)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_creditcard():\n",
    "    data = pd.read_csv(\"creditcard.csv\")\n",
    "    creditcard_target_feature = 'Class'\n",
    "    \n",
    "    data.drop('Time', axis=1, inplace=True)\n",
    "    \n",
    "    for column in data:\n",
    "        if column != creditcard_target_feature:\n",
    "            data[column] = binarization(data, column, creditcard_target_feature)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def report_generation(dataset_name, target_feature):\n",
    "    dataset_df = None\n",
    "    \n",
    "    train_df = None\n",
    "    test_df = None\n",
    "    \n",
    "    if(dataset_name == 'Telco'):\n",
    "        dataset_df = preprocess_telco()\n",
    "        train_df, test_df = split_dataset(dataset_df)  \n",
    "    elif(dataset_name == 'Adult'):\n",
    "        train_df = preprocess_adult('Train')\n",
    "        test_df = preprocess_adult('Test')\n",
    "    else:\n",
    "        dataset_df = preprocess_creditcard()\n",
    "        train_df, test_df = split_dataset(dataset_df)\n",
    "\n",
    "    decision_tree = make_tree(train_df, target_feature, len(train_df.columns))\n",
    "    h_5, z_5 = adaboost(train_df, train_df.columns[:-1], target_feature, decision_tree_learning, 5)\n",
    "    h_10, z_10 = adaboost(train_df, train_df.columns[:-1], target_feature, decision_tree_learning, 10)\n",
    "    h_15, z_15 = adaboost(train_df, train_df.columns[:-1], target_feature, decision_tree_learning, 15)\n",
    "    h_20, z_20 = adaboost(train_df, train_df.columns[:-1], target_feature, decision_tree_learning, 20)\n",
    "    \n",
    "    print(\"------------------------ Decision Tree ------------------------ \")\n",
    "    \n",
    "    train_accuracy_dt, train_tpr_dt, train_tnr_dt, train_ppv_dt, train_fdr_dt, train_f1_dt = test_dt(train_df, target_feature, decision_tree)\n",
    "    print(dataset_name + \"_train_accuracy: \", train_accuracy_dt)\n",
    "    print(dataset_name + \"_train_tpr: \", train_tpr_dt)\n",
    "    print(dataset_name + \"_train_tnr: \", train_tnr_dt)\n",
    "    print(dataset_name + \"_train_ppv: \", train_ppv_dt)\n",
    "    print(dataset_name + \"_train_fdr: \", train_fdr_dt)\n",
    "    print(dataset_name + \"_train_f1: \", train_f1_dt)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    test_accuracy_dt, test_tpr_dt, test_tnr_dt, test_ppv_dt, test_fdr_dt, test_f1_dt = test_dt(test_df, target_feature, decision_tree)\n",
    "    print(dataset_name + \"_test_accuracy: \", test_accuracy_dt)\n",
    "    print(dataset_name + \"_test_tpr: \", test_tpr_dt)\n",
    "    print(dataset_name + \"_test_tnr: \", test_tnr_dt)\n",
    "    print(dataset_name + \"_test_ppv: \", test_ppv_dt)\n",
    "    print(dataset_name + \"_test_fdr: \", test_fdr_dt)\n",
    "    print(dataset_name + \"_test_f1: \", test_f1_dt)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    print(\"------------------------  AdaBoost ------------------------ \")\n",
    "    \n",
    "    print(\"Number of boosting rounds, k = 5\")\n",
    "    \n",
    "    train_accuracy_adaboostk5 =  test_adaboost(train_df, target_feature, h_5, z_5)\n",
    "    print(dataset_name + \" train_accuracy: \", train_accuracy_adaboostk5)\n",
    "    \n",
    "    test_accuracy_adaboostk5 = test_adaboost(test_df, target_feature, h_5, z_5)\n",
    "    print(dataset_name + \" test_accuracy: \", test_accuracy_adaboostk5)\n",
    "    \n",
    "    print()\n",
    "    print(\"Number of boosting rounds, k = 10\")\n",
    "    \n",
    "    train_accuracy_adaboostk10 =  test_adaboost(train_df, target_feature, h_10, z_10)\n",
    "    print(dataset_name + \" train_accuracy: \", train_accuracy_adaboostk10)\n",
    "\n",
    "    test_accuracy_adaboostk10 = test_adaboost(test_df, target_feature, h_10, z_10)\n",
    "    print(dataset_name + \" test_accuracy: \", test_accuracy_adaboostk10)\n",
    "\n",
    "    print()\n",
    "    print(\"Number of boosting rounds, k = 15\")\n",
    "\n",
    "    train_accuracy_adaboostk15 =  test_adaboost(train_df, target_feature, h_15, z_15)\n",
    "    print(dataset_name + \" train_accuracy: \", train_accuracy_adaboostk15)\n",
    "\n",
    "    test_accuracy_adaboostk15 = test_adaboost(test_df, target_feature, h_15, z_15)\n",
    "    print(dataset_name + \" test_accuracy: \", test_accuracy_adaboostk15)\n",
    "\n",
    "    print()\n",
    "    print(\"Number of boosting rounds, k = 20\")\n",
    "    \n",
    "    train_accuracy_adaboostk20 =  test_adaboost(train_df, target_feature, h_20, z_20)\n",
    "    print(dataset_name + \" train_accuracy: \", train_accuracy_adaboostk20)\n",
    "\n",
    "    test_accuracy_adaboostk20 = test_adaboost(test_df, target_feature, h_20, z_20)\n",
    "    print(dataset_name + \" test_accuracy: \", test_accuracy_adaboostk20)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    \n",
    "report_generation('Telco', 'Churn')\n",
    "report_generation('Adult', 'label')\n",
    "report_generation('Credit Card', 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
